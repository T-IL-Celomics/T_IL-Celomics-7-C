# Cellomics-5-C Analysis Pipeline â€” User Guide

A step-by-step guide for running the Cellomics-5-C analysis pipeline through a graphical interface in your browser. **No coding knowledge is required** â€” you just click buttons and watch the results.

---

## Table of Contents

1. [What Does This Pipeline Do?](#1-what-does-this-pipeline-do)
2. [What You Need Before Starting](#2-what-you-need-before-starting)
3. [Setting Up the Environment (One-Time Setup)](#3-setting-up-the-environment-one-time-setup)
4. [Launching the App](#4-launching-the-app)
5. [Opening the App in Your Browser](#5-opening-the-app-in-your-browser)
6. [Configuring the Pipeline](#6-configuring-the-pipeline)
7. [Running the Pipeline Steps](#7-running-the-pipeline-steps)
8. [Understanding the Results](#8-understanding-the-results)
9. [Keeping the App Running After You Disconnect](#9-keeping-the-app-running-after-you-disconnect)
10. [Troubleshooting](#10-troubleshooting)

---

## 1. What Does This Pipeline Do?

This pipeline analyzes the movement and shape (morphokinetics) of cells over time. It takes raw cell tracking data and runs it through a series of analysis steps:

```
Step 0: Dose Extraction         â†’ Build per-cell dose-dependency summary from normalised Excel exports
Step 1: Data Preparation        â†’ Clean and filter the raw tracking data
Step 2: Feature Selection       â†’ Pick the most informative cell measurements
Step 3: Forecasting             â†’ AI models predict cell behavior to find patterns
Step 4: Embedding               â†’ Convert time-series data into compact representations
Step 5: Curve Fitting           â†’ Fit mathematical models to each cell's trajectory
Step 6: Clustering              â†’ Group similar cells together (with dose-category analysis)
Step 7: ANOVA                   â†’ Test if clusters are statistically different
Step 8: Descriptive Statistics  â†’ Summarize each cluster's characteristics
```

Steps 6â€“8 run in **two parallel branches**:
- **Embedding-only branch** â€” clusters cells using only the AI embeddings
- **Embedding + Fitting branch** â€” clusters cells using both AI embeddings and mathematical curve fits (richer information)

The **Run All** button executes steps in the correct dependency order: Dose Extraction runs first (before clustering), followed by Data Preparation through Descriptive Statistics.

---

## 2. What You Need Before Starting

| Requirement | Details |
|-------------|---------|
| **Remote Linux server** | With GPU(s) â€” needed for AI models in Steps 3 and 4 |
| **Python 3.11** | Already installed on most servers |
| **CUDA** | GPU drivers for PyTorch â€” check with `nvidia-smi` |
| **Java 17** | Needed for PySpark (used internally by the forecasting framework) |
| **Input data file** | Your pybatch output â€” either a `.csv` or `.xlsx` file |
| **Dose Excel files** (optional) | Normalised Excel exports (e.g. `Gab_Normalized_Combined_*.xlsx`) for dose analysis |

### Check GPU availability

Open a terminal on the server and type:
```bash
nvidia-smi
```
You should see your GPU(s) listed. If you get "command not found", CUDA is not installed.

### Check Java

```bash
java -version
```
If not installed:
```bash
sudo apt update && sudo apt install -y openjdk-17-jdk
```

---

## 3. Setting Up the Environment (One-Time Setup)

You only need to do this **once** on a new server.

### Step 3a: Copy the project folder

Copy the entire `T_IL-Cellomics-5-C_streamlit` folder to the server (using `scp`, FileZilla, or any file transfer tool).

### Step 3b: Fix Windows line endings

If the files were created or edited on Windows, run this to prevent hidden errors:
```bash
cd /path/to/T_IL-Cellomics-5-C_streamlit
sed -i 's/\r$//' *.py run_4gpus.sh
```

### Step 3c: Create the Python environment

**Option A â€” Creating a new virtual environment (recommended):**
```bash
python3.11 -m venv ~/mmf_gpu_env
source ~/mmf_gpu_env/bin/activate
pip install -r streamlit_requirements.txt
```

**Option B â€” Using an existing environment:**
```bash
source ~/mmf_gpu_env/bin/activate
pip install -r streamlit_requirements.txt
```

> **Note:** It is normal for the installation to take 10â€“20 minutes. If you see red warning text about version conflicts, that is usually fine â€” as long as the install finishes without a final error.

### Step 3d: Verify the setup

```bash
source ~/mmf_gpu_env/bin/activate
python -c "import torch; print('CUDA available:', torch.cuda.is_available())"
python -c "import streamlit; print('Streamlit version:', streamlit.__version__)"
python -c "import chronos; print('Chronos OK')"
```

All three should print without errors. The first one should say `CUDA available: True`.

---

## 4. Launching the App

Every time you want to use the pipeline:

```bash
source ~/mmf_gpu_env/bin/activate
cd /path/to/T_IL-Cellomics-5-C_streamlit
streamlit run app.py
```

You will see output like:
```
  You can now view your Streamlit app in your browser.
  Local URL:  http://localhost:8501
  Network URL: http://10.0.0.5:8501
```

**Do not close this terminal window** â€” the app stops when you close it (unless you use tmux/screen, see [Section 9](#9-keeping-the-app-running-after-you-disconnect)).

---

## 5. Opening the App in Your Browser

Since the server usually doesn't have a monitor, you need to access it from your personal computer.

### Method A: VS Code Remote SSH (Easiest)

1. Install the **Remote-SSH** extension in VS Code on your PC
2. Connect to the server via Remote-SSH
3. Open the project folder
4. Open a terminal inside VS Code and run `streamlit run app.py`
5. VS Code will show a notification â€” click **"Open in Browser"**
6. The app opens at `http://localhost:8501`

### Method B: SSH Tunnel

If you're not using VS Code:

**On your PC** (PowerShell, CMD, or Mac Terminal):
```bash
ssh -L 8501:localhost:8501 your_username@server-address
```

This opens an SSH session. In that session, start the app:
```bash
source ~/mmf_gpu_env/bin/activate
cd /path/to/T_IL-Cellomics-5-C_streamlit
streamlit run app.py
```

Then open `http://localhost:8501` in your browser.

### Method C: Direct Access (if on same network)

```bash
# On the server:
streamlit run app.py --server.address 0.0.0.0
```

Then open `http://<server-ip>:8501` in your browser.

---

## 6. Configuring the Pipeline

When the app loads, you'll see the **Pipeline Configuration** section at the top.

### 6a. Experiment ID

Type your experiment number (e.g. `008`). This controls the naming of output files like `Embedding008.json`.

### 6b. Path to your data file

Enter the **full path** to your input data file on the server. For example:
```
/home/username/T_IL-Cellomics-5-C_streamlit/cell_data/MergedAndFilteredExperiment008.csv
```

This can be a `.csv` or `.xlsx` file. A green checkmark âœ… will appear if the file is found.

### 6c. Treatment labels

Enter your treatment group names, separated by commas. For example:
```
CON0, BRCACON1, BRCACON2, BRCAOLAPARIB, BRCATALAZOPARIB
```

These labels are used for grouping cells in the clustering and statistical analysis steps.

### 6d. Dose CSV (optional)

If you have dose-response data, enter the path to the dose summary CSV. Leave blank to auto-detect in the `cell_data/` subdirectory. This CSV is generated by Step 0 (Dose Extraction).

### 6e. Control channel name

Enter the 4-letter channel code used as the control channel (default: `NNIR`). This channel is **excluded from dose-category labels** so that clustering only differentiates by treatment channels, not the control.

For example, if the experiment ID is `AM001100425CHR2D02293TGABYNNIRNOCONNN0NNN0WH00`, the channels are GABY (channel 1) and NNIR (channel 2). With control channel set to `NNIR`, the DoseLabel will only include GABY's category (e.g. `Cha1:High`) and skip NNIR's.

### 6f. Sidebar â€” Enable/Disable Steps

On the **left sidebar**, you'll see checkboxes for each pipeline step (including Dose Extraction). All are enabled by default. Uncheck any step you want to skip.

---

## 7. Running the Pipeline Steps

The main area shows **12 tabs** â€” one for each step. Work through them **from left to right**.

### Step 0: Dose Extraction

Builds a per-cell dose-dependency summary from normalised Excel exports. This step must run **before** clustering if you want dose-category analysis.

1. Click the **ðŸ’Š Dose Summary** tab
2. Expand **âš™ï¸ Dose Summary Parameters** to configure:
   - **Directory containing dose Excel files** â€” full path to the folder with your normalised Excel exports (e.g. `/data/exports/`). Leave blank to search in the current working directory
   - **Excel glob pattern** (default: `Gab_Normalized_Combined_*.xlsx`) â€” filename pattern resolved inside the directory above
   - **Sheet name** (default: `Area`) â€” the sheet to read in each workbook
   - **Filename prefix** (default: `Gab_Normalized_Combined_`) â€” prefix stripped from filename to derive well name
   - **Well-map JSON path** (optional) â€” path to a JSON file that maps short well names (like `B2`, `C3`) to full experiment IDs. Example contents:
     ```json
     {
       "B2": "AM001100425CHR2B02293TNNIRNOCONNN0NNN0NNN0WH00",
       "C2": "AM001100425CHR2C02293TMETRNNIRNOCONNN0NNN0WH00"
     }
     ```
     Leave blank to use the built-in default mapping.
3. Click **ðŸ’Š Run Dose Extraction**
4. Output: `dose_dependency_summary_all_wells.csv`

### Step 1: Data Preparation

1. Click the **ðŸ“ Data Preparation** tab
2. (Optional) Expand **âš™ï¸ Parameters** to change:
   - **Min frames per cell** (default: 25) â€” cells with fewer time points are removed
   - **Max gap** (default: 5) â€” largest allowed gap in a cell's time series
3. Click **ðŸš€ Run Data Preparation**
4. Wait for it to finish â€” you'll see a green "Completed" message
5. Output: `raw_all_cells.csv` in the `cell_data/` folder

### Step 2: Feature Selection

1. Click the **ðŸŽ¯ Feature Selection** tab
2. (Optional) Adjust parameters like PCA variance threshold, correlation cutoff, etc.
3. Click **ðŸŽ¯ Run Feature Selection** button
4. Output: `selected_features.txt` and figures showing which features were selected

### Step 3: Forecasting

This step uses AI models to evaluate how well they can predict cell behavior. It **does not** change your data â€” it only determines which AI model works best for each feature.

1. Click the **ðŸ“ˆ Forecasting** tab
2. Choose **Single GPU** or **Multi-GPU Parallel** mode:
   - Single GPU: Simpler, runs on one GPU
   - Multi-GPU: Faster with multiple GPUs â€” choose how many GPUs to use
3. (Optional) Change **Max cells** (default: 500). This subsamples cells for faster evaluation. Use 0 for all cells (slower but slightly more precise model ranking).
4. Click **ðŸ“ˆ Run Forecasting**
5. â³ This is the **longest step** â€” can take 1â€“4 hours depending on data size and GPU count
6. Output: Three JSON files mapping each feature to its best-performing model:
   - `best_model_per_feature.json` â€” best model overall (any family)
   - `best_chronos_model_per_feature.json` â€” best Chronos model (T5 or Bolt)
   - `best_t5_model_per_feature.json` â€” best T5 model (used by the Embedding step)

### Step 4: Embedding

This step uses the best T5 models (from Step 3) to convert each cell's time-series data into a compact numerical representation (embedding), then reduces dimensions with UMAP.

1. Click the **ðŸ§¬ Embedding** tab
2. Choose **Single GPU** or **Multi-GPU Parallel** mode
3. (Optional) Change **UMAP dimensions** (default: 3)
4. Click the Run button
5. Output: `Embedding008.json` (or your experiment ID) in the `embeddings/` folder

### Step 5: Curve Fitting

Fits 12 mathematical models (linear, exponential, logistic, etc.) to each cell's trajectory for each feature.

1. Click the **ðŸ“ Curve Fitting** tab
2. (Optional) Adjust **maxfev** (max iterations for curve fitting) and **p-value threshold**
3. Click **ðŸ“ Run Curve Fitting**
4. Output: Multiple CSV and JSON files in the project root, plus trajectory plots in `figures/`

### Step 6: Clustering

Groups cells into clusters based on their embeddings (and optionally fitting parameters). Dose-category analysis is included automatically if a dose CSV exists.

There are **two clustering tabs**:

- **ðŸ”® Clustering** â€” uses embeddings only
- **ðŸ”— Emb+Fit Clustering** â€” uses embeddings combined with curve fitting parameters (richer analysis)

For each:
1. Click the tab
2. (Optional) Adjust k-range (number of clusters to try), PCA components, etc.
3. Click the Run button
4. Output: Cluster assignments, PCA scatter plots (by cluster, treatment, and dose category), contingency tables, heatmaps

**Dose-category plots:** When dose data is available, the clustering step generates PCA scatter plots and contingency tables split by dose category. The control channel (set in Pipeline Configuration) is automatically excluded from these dose labels so plots only show treatment-relevant channel categories.

### Step 7: ANOVA

Tests whether the clusters are statistically different for each feature.

- **ðŸ“Š ANOVA** tab â€” for embedding-only clusters
- **ðŸ“Š Emb+Fit ANOVA** tab â€” for combined clusters

Output: Excel files with p-values highlighted in blue where significant.

### Step 8: Descriptive Statistics

Summarizes each cluster with mean, standard deviation, standard error, and confidence intervals.

- **ðŸ“‹ Descriptive Stats** tab â€” for embedding-only clusters
- **ðŸ“‹ Emb+Fit Descriptive** tab â€” for combined clusters

Output: Excel files with descriptive statistics per cluster.

### Run All Stages

Above the tabs, there's a **ðŸš€ Run All Enabled Stages** button that runs all enabled steps in sequence. The execution order is: Dose Extraction â†’ Data Preparation â†’ Feature Selection â†’ Forecasting â†’ Embedding â†’ Curve Fitting â†’ Clustering â†’ ANOVA â†’ Descriptive Stats (then the Emb+Fit branch).

It has a **â­ï¸ Skip stages whose outputs already exist** checkbox (on by default) â€” if outputs from a step already exist, that step is skipped automatically. There's also a **ðŸ›‘ Stop** button to cancel during execution.

---

## 8. Understanding the Results

### Where are the output files?

All outputs are saved in the project directory on the server:

| Step | Output Files | Location |
|------|-------------|----------|
| Dose Extraction | `dose_dependency_summary_all_wells.csv` | Root |
| Data Prep | `raw_all_cells.csv` | `cell_data/` |
| Feature Selection | `selected_features.txt`, heatmaps/plots | Root + `figures/` |
| Forecasting | `best_model_per_feature.json`, `best_t5_model_per_feature.json` | Root |
| Embedding | `Embedding{ID}.json` | `embeddings/` |
| Curve Fitting | `fitting_*.csv`, `fitting_*.json`, trajectory plots | Root + `figures/` |
| Clustering | Cluster CSVs, PCA plots, dose tables, heatmaps | `clustering/` |
| Emb+Fit Clustering | Combined cluster CSVs, PCA plots, descriptive tables | `fitting/` |
| ANOVA | `ANOVA - OneWay.xlsx` | Root |
| Emb+Fit ANOVA | `embedding_fitting_ANOVA - OneWay.xlsx` | Root |
| Descriptive Stats | `Descriptive_Table_By_Cluster_UniqueCells.xlsx` | Root |
| Emb+Fit Descriptive | `embedding_fitting_Descriptive_Table_By_Cluster_UniqueCells.xlsx` | Root |

### Viewing results in the app

After each step finishes, the app shows:
- **Output images** â€” plots and figures are displayed directly in the browser
- **Completion status** â€” green success messages with timing information
- **Script output** â€” expand the output section to see detailed logs

### Downloading results

To get the files onto your personal computer:
- **VS Code**: Right-click the file in the Explorer sidebar â†’ Download
- **SCP**: `scp username@server:/path/to/file.xlsx ./`
- **FileZilla**: Navigate to the folder and drag files to your PC

---

## 9. Keeping the App Running After You Disconnect

By default, closing your terminal or SSH connection stops the app. To keep it running:

### Using tmux (recommended)

```bash
# Start a named session
tmux new -s streamlit

# Inside tmux, run the app
source ~/mmf_gpu_env/bin/activate
cd /path/to/T_IL-Cellomics-5-C_streamlit
streamlit run app.py --server.headless true

# Detach: press Ctrl+B, then D
# The app keeps running in the background

# Reconnect later:
tmux attach -t streamlit
```

### Using screen

```bash
screen -S streamlit
source ~/mmf_gpu_env/bin/activate
cd /path/to/T_IL-Cellomics-5-C_streamlit
streamlit run app.py --server.headless true
# Detach: press Ctrl+A, then D
# Reconnect: screen -r streamlit
```

### Using nohup

```bash
nohup streamlit run app.py --server.port 8501 --server.headless true > streamlit.log 2>&1 &
```

---

## 10. Troubleshooting

### "File not found" errors

- Make sure the path you entered in Pipeline Configuration is the **full path** (starting with `/`)
- Check that the file actually exists: `ls -la /path/to/your/file.csv`

### "No files matched the glob pattern" (Dose Extraction)

- Make sure the **Directory containing dose Excel files** is set to the correct folder path
- Check that the glob pattern matches your file names (e.g. `Gab_Normalized_Combined_*.xlsx`)
- Verify files exist: `ls /path/to/dose/dir/Gab_Normalized_Combined_*.xlsx`

### The app shows "Step X requires outputs from Step Y"

- Steps depend on each other â€” you must run them in order
- Dose Extraction should run before Clustering if you want dose analysis
- Check the "Pipeline Intermediate Files" expander to see which files exist

### `\r: command not found` or strange errors after copying files from Windows

Run this once to fix line endings:
```bash
sed -i 's/\r$//' *.py run_4gpus.sh
```

### GPU not detected

```bash
# Check GPU is visible
nvidia-smi

# Check PyTorch can see it
python -c "import torch; print(torch.cuda.is_available())"
```

If `False`, your PyTorch installation may not match your CUDA version.

### "No module named ..." errors

Make sure you activated the right Python environment:
```bash
source ~/mmf_gpu_env/bin/activate
```

### Forecasting takes too long

- Use **Multi-GPU** mode if you have multiple GPUs
- Reduce **Max cells** (e.g. from 500 to 200) for faster model evaluation
- The model ranking is usually stable even with fewer cells

### Out of GPU memory

- Reduce the number of active models in `my_models_conf.yaml` (remove larger models like `ChronosT5Large`, `MoiraiLarge`)
- Reduce **Max cells** in the Forecasting parameters

### Matplotlib / display errors

Already handled â€” all scripts run in headless mode. If you still see display errors, set:
```bash
export MPLBACKEND=Agg
```

### The app is slow to respond / shows "Running..."

This is normal during long computations. The app processes data in the background. Do not refresh the page â€” wait for the step to finish.

### Mixed-type column warnings

If you see `DtypeWarning: Columns have mixed types`, this is handled automatically â€” the pipeline reads data with `low_memory=False` and filters to numeric columns only for analysis.

---

## Quick Reference: Running the Pipeline Start to Finish

```bash
# 1. Connect to server
ssh username@server

# 2. Activate environment
source ~/mmf_gpu_env/bin/activate

# 3. Go to project folder
cd /path/to/T_IL-Cellomics-5-C_streamlit

# 4. Launch the app
streamlit run app.py

# 5. Open in browser (on your PC)
#    â†’ http://localhost:8501

# 6. In the app:
#    a. Enter your data file path
#    b. Enter treatment labels
#    c. Set control channel name (default: NNIR)
#    d. (Optional) Configure dose extraction directory
#    e. Click through tabs 0â€“8, or use "Run All"
#    f. Download results
```

---

## Pipeline Files Reference

### Active pipeline scripts

| File | Purpose |
|------|---------|
| `app.py` | Main Streamlit GUI â€” runs all steps |
| `build_dose_summary.py` | Step 0: Dose extraction from Excel exports |
| `make_raw_all_cells_from_pybatch.py` | Step 1: Data preparation |
| `feature_selection.py` | Step 2: Feature selection |
| `TSA_analysis.py` | Step 3: Forecasting (single GPU) |
| `TSA_analysis_4gpu.py` | Step 3: Forecasting (multi-GPU) |
| `Embedding.py` | Step 4: Embedding (single GPU) |
| `Embedding_multi_gpu.py` | Step 4: Embedding (multi-GPU) |
| `fit_cell_trajectory.py` | Step 5: Curve fitting |
| `embedding_unsupervised_clustering.py` | Step 6: Clustering (embedding only) |
| `embedding_fitting_unsupervised_clustering.py` | Step 6b: Clustering (embedding + fitting) |
| `ANOVA.py` | Step 7: ANOVA |
| `embedding_fitting_anova.py` | Step 7b: ANOVA (embedding + fitting) |
| `descriptive_table_by_cluster.py` | Step 8: Descriptive statistics |
| `embedding_fitting_descriptive_table.py` | Step 8b: Descriptive statistics (embedding + fitting) |

### Configuration files

| File | Purpose |
|------|---------|
| `my_models_conf.yaml` | AI model configuration for forecasting |
| `well_map.json` | Maps short well names to full experiment IDs (for dose extraction) |
| `streamlit_requirements.txt` | Python package dependencies |
| `run_4gpus.sh` | Shell helper for multi-GPU forecasting |
